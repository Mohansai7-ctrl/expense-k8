#using configMap as volume for deployment as if nginx.conf is changed instead of changing in docker(if changed in docker, it needs to be rebuild and agian to be deployed), configured as configMap data and used it as volume in configuration code of deployment, so that if any changes occurs, just restart of pods(deleting the pods manually then deployment replicas will take care of creation again) is enough.
apiVersion: v1
kind: ConfigMap
metadata:
  name: frontend
  namespace: expense
data:
  nginx.conf: | # | represents multiple lines
    user www-data;
    worker_processes 4;
    pid /var/run/nginx.pid;

    events {
      worker_connections 768;
      # multi_accept on;
    }

    http {

      ##
      # Basic Settings
      ##

      sendfile on;
      tcp_nopush on;
      tcp_nodelay on;
      keepalive_timeout 65;
      types_hash_max_size 2048;
      large_client_header_buffers 6 32k;
      client_max_body_size 100m;

      # server_names_hash_bucket_size 64;
      # server_name_in_redirect off;
      include /etc/nginx/mime.types;
      default_type application/octet-stream;

      ##
      # Logging Settings
      ##
      access_log /var/log/nginx/access.log;
      error_log /var/log/nginx/error.log debug; # change from debug to warn or error for production

      ##
      # Gzip Settings
      ##
      gzip on;
      gzip_disable "msie6";

      ##
      # Virtual Host Configs
      ##

      include /etc/nginx/conf.d/*.conf;
      include /etc/nginx/sites-enabled/*;

      server {
        listen       80;
        server_name  localhost;

        proxy_http_version 1.1;

        #charset koi8-r;
        #access_log  /var/log/nginx/host.access.log  main;
        #error_log /dev/stdout debug;
        #rewrite_log on;

        location / {
            root   /usr/share/nginx/html;
            index  index.html index.htm;
            ssi    on;
        }

        location /images/ {
            expires 5s;
            root   /usr/share/nginx/html;
            try_files $uri /images/placeholder.png;
        }

        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   /usr/share/nginx/html;
        }
        
        location /api/ { 
            proxy_pass http://backend:8080/;
        }

        }

    }

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: expense
  labels:
    app: frontend
    tier: web
    project: expense
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
      tier: web
      project: expense
  template:
    metadata:
      name: frontend
      labels:
        app: frontend
        tier: web
        project: expense
    spec:
      containers:
      - name: frontend
        image: mohansai7/frontend:v1
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 100m
            memory: 128Mi
        volumeMounts:
        - name: nginx-conf
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
          readOnly: true
      volumes:
      - name: nginx-conf
        configMap:
          name: frontend
          items:
            - key: nginx.conf   #it will check and get the key name of nginx.conf from configMap from path nginx.conf mentiond from configMap data
              path: nginx.conf
        
---
apiVersion: v1
kind: Service
metadata:
  name: frontend   #name should be same as above replicas creation
  namespace: expense
spec:
  type: LoadBalancer   #When TargetGroupBinding used for Ingress ALB, this type LoadBalancer(Classic Load Balancer) to be commented
  selector:
    app: frontend
    tier: web
    project: expense
  ports:
  - name: frontend
    protocol: TCP
    port: 80
    targetPort: 80

---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: frontend
  namespace: expense
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 15
  #for memory utilization metrics, use metrics block instead/unlike targetCPUUtilizationPercentage
  # metrics:
  #   - type: Resource
  #     resource:
  #       name: memory
  #       target:
  #         type: Utilization
  #         averageUtilization: 80

  #--- 
  #creating TargetGroupBinding for terraform-aws-eks alb:
  # apiVersion: elbv2.k8s.aws/v1beta1
  # kind: TargetGroupBinding
  # metadata:
  #   name: frontend
  #   namespace: expense
  # spec:
  #   specRef:
  #     name: frontend
  #     port: 80
  #   targetGroupARN:  #after creation of 60-alb in terraform-aws-eks, we need to give created targetGroup arn here
  #   targetType: ip